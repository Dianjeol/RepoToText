*GitHub Repository "spartanhaden/CaseConnect"*

'''--- README.md ---
# CaseConnect
This project is a semantic search engine for the [NamUs](https://www.namus.gov/MissingPersons/Search) missing persons database. It contains tools to scrape all the missing person cases and download all the associated images with each case. It also contains tools to embed the text and images into a vector space. The goal is to be able to search the database by text or image and get back similar cases. This could be useful for law enforcement to search for similar cases to a new case or for the public to improve the ability to search if someone is in the database.

## TODO

- [x] write a scraper for the database
- [x] embed all the text
- [x] embed the images
- [ ] build the front end
- [ ] switch to embedding db from sklearn nn
- [ ] remove extra json data to improve embedding cost and relevance
- [ ] test search by image
- [ ] move cali db scraper to seperate file as there are SSL issues with their db

stretch goals

- [ ] use control net to convert sketches to images and then do image search on those semantically
- [ ] live generations from the sketches and then doing semantic search so you can see people as you draw
- [ ] add chat to prompt user for more details if the input is not very descriptive

## cost

$0.0004 / 1K tokens
34229931 tokens
$13.6919724

## data

| data | filetype | description | embeddings |
|------|----------|-------------|------------|
| json_cases | json | raw data | |
| case_images | jpg | raw data | |
| text_embeddings | json | embedded json_cases | text-embedding-ada-002 |
| image_embeddings | json | embedded case_images | ViT-bigG-14 |
| search_text | user input | user input text | ada-002 and ViT-bigG-14 |
| search_image | user input | user input image | ViT-bigG-14 |

'''
'''--- clipper.py ---
#!/usr/bin/env python3

import json
import os
import time

import numpy as np
import open_clip
import torch
from IPython import embed
from PIL import Image

class Clipper:
    def __init__(self, device='cpu'):
        self.device = device
        # self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        start_time = time.time()
        print("loading model...")
        self.model, _, self.preprocess = open_clip.create_model_and_transforms('ViT-bigG-14', pretrained='laion2b_s39b_b160k', device=self.device)
        # model, _, preprocess = open_clip.create_model_and_transforms('ViT-bigG-14', pretrained='laion2b_s39b_b160k')
        # model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')
        print("model loaded in %0.2f seconds" % (time.time() - start_time))
        self.tokenizer = open_clip.get_tokenizer('ViT-bigG-14')

    def embed_image(self, case_id, image_id):
        # check if the image embedidngs have already been saved
        if os.path.isfile(f'image_embeddings/{case_id}_{image_id}.json'):
            # print(f'image {case_id}_{image_id} has already been embedded')
            return False

        image_path = f'case_images/{case_id}_{image_id}.jpg'
        try:
            image = Image.open(image_path)
            converted_image = self.preprocess(image.convert("RGB"))
        except:
            print(f'error opening {image_path}')
            return False

        # start_time = time.time()
        # print('creating embeddings...')
        with torch.no_grad(), torch.cuda.amp.autocast():
            tensor_image = torch.stack([converted_image.to(self.device)])
            embedding = self.model.encode_image(tensor_image).float()
            # normalize
            embedding /= embedding.norm(dim=-1, keepdim=True)

            # print("embeddings created in %0.2f seconds" % (time.time() - start_time))

            output = embedding.cpu().numpy()[0].tolist()

            # save the embedding to a file
            with open(f'image_embeddings/{case_id}_{image_id}.json', "w") as f:
                json.dump(output, f)
        return True

    def embed_raw_image(self, image):
        converted_image = self.preprocess(image.convert("RGB"))

        # start_time = time.time()
        # print('creating embeddings...')
        with torch.no_grad(), torch.cuda.amp.autocast():
            tensor_image = torch.stack([converted_image.to(self.device)])
            embedding = self.model.encode_image(tensor_image).float()
            # normalize
            embedding /= embedding.norm(dim=-1, keepdim=True)

            # print("embeddings created in %0.2f seconds" % (time.time() - start_time))

            output = embedding.cpu().numpy()[0].tolist()

        # TODO test this

        return output

    def embed_text(self, query):
        start_time = time.time()
        text = self.tokenizer([query]).to(self.device)
        text_embedding = None
        with torch.no_grad(), torch.cuda.amp.autocast():
            text_embedding = self.model.encode_text(text).float()
            text_embedding /= text_embedding.norm(dim=-1, keepdim=True)
        print(f"encoded text in {time.time() - start_time} seconds")

        normie_embedding = text_embedding.cpu().numpy()
        return normie_embedding

        # with torch.no_grad(), torch.cuda.amp.autocast():
        #     text = self.tokenizer([text]).to(self.device)
        #     embedding = self.model.encode_text(text).float()
        #     # normalize
        #     embedding /= embedding.norm(dim=-1, keepdim=True)

        #     output = embedding.cpu().numpy()[0].tolist()

        #     # save the embedding to a file
        #     with open(f'text_embeddings/{case_id}.json', "w") as f:
        #         json.dump(output, f)
        # return True

    # def search_images(self, query=''):
    #     text = self.tokenizer([query]).to(self.device)
    #     text_embedding = None
    #     with torch.no_grad(), torch.cuda.amp.autocast():
    #         text_embedding = self.model.encode_text(text).float()
    #         text_embedding /= text_embedding.norm(dim=-1, keepdim=True)

    #     # similarities = [text_embedding @ img_embedding.T for img_embedding in embeddings.values()]
    #     # similarity = text_embedding.cpu().numpy() @ image_features.cpu().numpy().T
    #     similarity = text_embedding @ image_features.T

    #     for i in range(len(similarity[0])):
    #         print(f"{input_keys[i]}: {similarity[0][i]*100.0:.2f}%")

    #     print()

    #     # print the similarity with the highest value but in a cuda safe way
    #     print(f"most similar image to {query}: {input_keys[np.argmax(similarity.cpu().numpy())]} - {np.max(similarity.cpu().numpy())*100.0:.2f}%")

    #     # print(f"most similar image to {query}: {input_keys[np.argmax(similarity)]} - {np.max(similarity)*100.0:.2f}%")

    #     # 1280 long

    #     embed()

    #     print()
    #     search_images('scooter')

    #     # take use your input and then search for it continuously until i exit the program
    #     while True:
    #         query = input("enter a query: ")
    #         if query == 'exit':
    #             break
    #         search_images(query)

'''
'''--- namus_scraper.py ---
#!/usr/bin/env python3

import glob
import json
import os
import sys
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

import openai
import requests
from dotenv import load_dotenv
from IPython import embed

from clipper import Clipper
from search import Search

class NamusScraper:
    def __init__(self):
        load_dotenv()

        openai.api_key = os.getenv('OPENAI_API_KEY')

        self.base_url = "https://www.namus.gov"
        self.case_url = self.base_url + "/api/CaseSets/NamUs/MissingPersons/Cases/"
        self.downloaded_cases = []
        self.downloaded_images = []

        # use glob to get the list of all the json files in the json_cases folder
        case_names = glob.glob("json_cases/*.json")

        # add the case id to the list of downloaded cases
        for case_name in case_names:
            case_id = case_name.split("/")[1].split(".")[0]
            self.downloaded_cases.append(case_id)

        self.downloaded_cases.sort()

        print(f'found {len(self.downloaded_cases)} downloaded cases')

        image_names = glob.glob("case_images/*")

        for image_name in image_names:
            # 'image_id'_'case_id'
            image_id_pair = image_name.split("/")[1].split(".")[0]
            self.downloaded_images.append(image_id_pair.split("_"))

        print(f'found {len(self.downloaded_images)} downloaded images')

    def get_case_from_api(self, case_id):
        try:
            response = requests.get(f"{self.case_url}{case_id}")
            response.raise_for_status()
            case = response.text

            # save it to a file
            with open(f"json_cases/{case_id}.json", "w") as f:
                f.write(case)

            return case_id
        except requests.exceptions.RequestException as e:
            return f"An error occurred while downloading the web page: {e}"

    def get_image_links_from_case(self, case_id):
        # open the json file
        with open(f"json_cases/{case_id}.json", "r") as f:
            case = f.read()

        # parse the json file
        case = json.loads(case)

        # get the images from the case
        images = case["images"]

        # get the links from the images
        links = [image["hrefDownload"] for image in images]

        # extract the image ids and return them
        return [link.split("/")[-2] for link in links]

    def download_image(self, case_id, image_id):
        image_download_url = self.base_url + f'/api/CaseSets/NamUs/MissingPersons/Cases/{case_id}/Images/{image_id}/Download'

        try:
            response = requests.get(image_download_url)
            response.raise_for_status()
            image = response.content

            # save it to case_images/{case_id}_{image_id}
            with open(f"case_images/{case_id}_{image_id}.jpg", "wb") as f:
                f.write(image)

            return case_id, image_id
        except requests.exceptions.RequestException as e:
            return f"An error occurred while downloading the web page: {e}"

    def download_new_images(self):
        # get the links from all the self.downloaded_cases
        images = []
        num_threads = 16
        futures = []

        for case_id in self.downloaded_cases:
            for image_id in self.get_image_links_from_case(case_id):
                images.append((case_id, image_id))

        print(f'found {len(images)} images in json')

        images_to_download = []

        for image in images:
            case_id = image[0]
            image_id = image[1]

            # check if the image has already been downloaded
            if [case_id, image_id] not in self.downloaded_images:
                images_to_download.append(image)

        # Create a ThreadPoolExecutor with 8 worker threads
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            # Submit tasks to the executor
            for case_id, image_id in images_to_download:
                future = executor.submit(self.download_image, case_id, image_id)
                futures.append(future)

            # Wait for all tasks to complete and handle their results
            for future in as_completed(futures):
                case_id, image_id = future.result()

    def download_new_pages(self):
        # set to the last case id that was downloaded
        starting_case_id = int(self.downloaded_cases[-1])
        num_threads = 16
        futures = []

        # Create a ThreadPoolExecutor with 8 worker threads
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            # Submit tasks to the executor
            for case_id in range(starting_case_id, starting_case_id + 1000):
                future = executor.submit(self.get_case_from_api, case_id)
                futures.append(future)

            # Wait for all tasks to complete and handle their results
            for future in as_completed(futures):
                case_id = future.result()
                print(f'downloaded case {case_id}')

    def tokenize_all_the_files(self):
        import tiktoken
        enc = tiktoken.get_encoding("cl100k_base")

        # get the list of all the json files in the json_cases folder
        case_names = glob.glob("json_cases/*.json")

        token_totel = 0

        for i, case_name in enumerate(case_names):
            if i % 100 == 0:
                print(f'case {i}')
            with open(case_name, "r") as f:
                case = f.read()

                # tokenize the case
                case_tok = enc.encode(case)

                token_totel += len(case_tok)

                if len(case_tok) > 8000:
                    print(f'case {case_name} is too long')
                    print(f'len(case_tok) = {len(case_tok)}')

        print(f'token_totel = {token_totel}')

    def embed_all_the_case_files(self):
        # get the list of all the json files in the json_cases folder
        case_names = glob.glob("json_cases/*.json")

        for i, case_name in enumerate(case_names):
            case_id = case_name.split("/")[-1].split(".")[0]
            self.embed_file(case_id)

            # print the fraction and the percentage complete two decimal places
            print(f'{i + 1}/{len(case_names)} = {i/len(case_names):.2%}')

    def embed_file(self, case_id):
        # check if the file has already been embedded
        if os.path.exists(f"text_embeddings/{case_id}.json"):
            # print(f'case {case_id} has already been embedded')
            return

        # get the json file
        with open(f"json_cases/{case_id}.json", "r") as f:
            case = f.read()

        embedding = openai.Embedding.create(input=case, model="text-embedding-ada-002")["data"][0]["embedding"]

        # save the embedding to a file
        with open(f"text_embeddings/{case_id}.json", "w") as f:
            f.write(json.dumps(embedding))

        print(f'embedded case {case_id}')

if __name__ == "__main__":
    scraper = NamusScraper()
    # scraper.download_new_pages()
    # scraper.download_new_images()
    # scraper.tokenize_all_the_files()
    # scraper.embed_all_the_case_files()

    clipper = Clipper('cpu')

    search = Search(clipper)
    # search = Search(None)

    search.run()

    # count = 0
    # start_time = time.time()

    # for i, (case_id, image_id) in enumerate(scraper.downloaded_images):
    #     successful = clipper.embed_image(case_id, image_id)
    #     if successful:
    #         count += 1

    #     # print the image rate of count since start_time
    #     rate = count / (time.time() - start_time)

    #     print(f'{i + 1}/{len(scraper.downloaded_images)} = {i/len(scraper.downloaded_images):.2%} - {rate:.2f} images/s')

'''
'''--- run.sh ---
#!/bin/bash

clear
clear

autopep8 -i *.py --max-line-length 200
#pylint --max-line-length=200 main.py

# ./main.py
./namus_scraper.py

'''
'''--- search.py ---
#!/usr/bin/env python3

import glob
import json
import subprocess
import os
import time

import numpy as np
import open_clip
import torch
from IPython import embed
from PIL import Image
from sklearn.neighbors import NearestNeighbors
from torchinfo import summary
import openai

class Search:
    def __init__(self, clipper):
        self.clipper = clipper

        self.knn_amount = 10

        self.text_embedding_and_case_id_list = self.load_text_embeddings()
        self.image_embedding_and_case_id_and_image_id_list = self.load_image_embeddings()

        self.text_nearest_neighbors_ada_2 = self.load_text_nearest_neighbors()
        self.image_nearest_neighbors_open_clip = self.load_image_nearest_neighbors()

    def load_text_embeddings(self):
        # make list of pairs of embedding and case_id that is indices
        text_embedding_and_case_id = []

        embedding_file_paths = glob.glob('text_embeddings/*.json')
        embedding_file_paths.sort()

        for filepath in embedding_file_paths:
            case_id = int(filepath.split('/')[-1].split('.')[0])

            with open(filepath, 'r') as f:
                embedding = json.load(f)

                text_embedding_and_case_id.append((embedding, case_id))

        # sort by case id
        text_embedding_and_case_id.sort(key=lambda x: x[1])

        # print how many embeddings there are
        print(f'number of text embeddings: {len(text_embedding_and_case_id)}')

        return text_embedding_and_case_id

    def load_image_embeddings(self):
        # make a list of pairs of embedding and case_id and image_id
        image_embedding_and_case_id_and_image_id = []

        image_embedding_file_paths = glob.glob('image_embeddings/*.json')
        image_embedding_file_paths.sort()

        for filepath in image_embedding_file_paths:
            case_id = int(filepath.split('/')[-1].split('_')[0])
            image_id = int(filepath.split('/')[-1].split('_')[1].split('.')[0])

            with open(filepath, 'r') as f:
                embedding = json.load(f)

                image_embedding_and_case_id_and_image_id.append((embedding, case_id, image_id))

        # sort by case id and image id
        image_embedding_and_case_id_and_image_id.sort(key=lambda x: (x[1], x[2]))

        # print how many embeddings there are
        print(f'number of image embeddings: {len(image_embedding_and_case_id_and_image_id)}')

        return image_embedding_and_case_id_and_image_id

    def load_text_nearest_neighbors(self):
        formatted_embeddings_list_np = np.array([np.array(embedding) for embedding, _ in self.text_embedding_and_case_id_list])

        start_time = time.time()
        nearest_neighbors = NearestNeighbors(n_neighbors=self.knn_amount, metric='cosine', algorithm='auto').fit(formatted_embeddings_list_np)
        print(f'created NearestNeighbors instance in {time.time() - start_time} seconds')

        return nearest_neighbors

    def load_image_nearest_neighbors(self):
        formatted_embeddings_list_np = np.array([np.array(embedding) for embedding, _, _ in self.image_embedding_and_case_id_and_image_id_list])

        start_time = time.time()
        nearest_neighbors = NearestNeighbors(n_neighbors=self.knn_amount, metric='cosine', algorithm='auto').fit(formatted_embeddings_list_np)
        print(f'created NearestNeighbors instance in {time.time() - start_time} seconds')

        return nearest_neighbors

    def search_with_text_clip(self, text):
        # get the embedding
        text_embedding = self.clipper.get_text_embedding(text)

        # get the image nearest neighbors
        distances, indices = self.image_nearest_neighbors_open_clip.kneighbors([text_embedding])

        # get a list of length self.knn_amount of pairs of case_id and image_id from image_embedding_and_case_id_and_image_id_list
        case_id_and_image_id_list = [self.image_embedding_and_case_id_and_image_id_list[index][1:] for index in indices[0]]

        print(f'case_id_and_image_id_list: {case_id_and_image_id_list}')

        # TODO test this and logic

    def search_with_image_clip(self, image_filepath):
        # get the image
        try:
            image = Image.open(image_filepath)
        except:
            print(f'could not open image at {image_filepath}')
            return

        # get the embedding
        image_embedding = self.clipper.embed_raw_image(image)

        # get the text nearest neighbors
        distances, indices = self.text_nearest_neighbors_ada_2.kneighbors([image_embedding])

        # get a list of length self.knn_amount of case_ids from text_embedding_and_case_id_list
        case_id_list = [self.text_embedding_and_case_id_list[index][1] for index in indices[0]]

        print(f'case_id_list: {case_id_list}')

        # TODO test this and logic

    def search_with_text_ada_2(self, text):
        embedding = openai.Embedding.create(input=text, model="text-embedding-ada-002")["data"][0]["embedding"]

        # get the image nearest neighbors
        distances, indices = self.text_nearest_neighbors_ada_2.kneighbors([embedding])

        # get a list of length self.knn_amount of pairs of case_id from self.text_embedding_and_case_id_list
        for i in range(self.knn_amount):
            index = indices[0][i]
            current_match = self.text_embedding_and_case_id_list[index]

            case_id = current_match[1]

            distance = distances[0][i]

            # print the distance in green
            print(f'distance - \033[92m{distance}\033[0m')
            # print the case id in cyan
            print(f'case_id - \033[96m{case_id}\033[0m')

            # load the json file
            with open(f'json_cases/{case_id}.json', 'r') as f:
                case_json = json.load(f)
                print(f'case_json: {case_json}')

        embed()

        # TODO test this and logic

    def run(self):
        while True:
            print()
            query = input("enter a query type: ('text-to-clip', 'text-to-ada', 'image-to-clip' 'exit') ")
            if query == 'exit':
                print('exiting')
                break
            elif query == 'text-to-clip':
                query = input("enter a search term: ")
                self.search_with_text_clip(query)
            elif query == 'text-to-ada':
                query = input("enter a search term: ")
                self.search_with_text_ada_2(query)
            elif query == 'image-to-clip':
                filepath = input("enter an image path : ")
                self.search_with_image_clip(filepath)
            else:
                print('invalid query type')
                continue

'''